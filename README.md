# kafka-data-engineering-project-with-stockmarket-data

#### Steps:
- Created an EC2 instance on AWS cloud.
- Connected to the EC2 instance from a local computer using SSH.
- Installed Kafka, Java, and other necessary packages on the EC2 instance.
- Started the Zookeeper and Kafka server on the EC2 instance.
- Created a Kafka topic to start the data processing procedure.
- Set up a Kafka consumer to receive and process the data from the Kafka topic.
- Stored the consumer data on an S3 bucket after connecting to it.
- Created a Glue Crawler to query and analyze the data on the Athena editor.
- Stored the query results in a database for further analysis using the Glue Catalog and Athena.
#### Dataset:
In this project, I have used the stock market indexProcessed data (reference: https://www.kaggle.com/datasets/ramamet4/nse-stocks-database)

#### Skills used:
Python for programming, Kafka for data streaming, AWS EC2 for cloud computing, S3 for data storage, Glue Crawler for data querying and analysis, Glue Catalog for data cataloging, and Athena for data querying and analysis.
